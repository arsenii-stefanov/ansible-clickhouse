---

### Installation on host is preferrable. Installation in Docker has not been properly tested so use at your own risk
clickhouse_installation_type: "docker"        # Available options: host, docker

clickhouse_cluster_name: "clickhouse_cluster"
clickhouse_node_hostname: "{{ ansible_facts.fqdn }}"    # This should be a valid DNS (or IP, but DNS is preferrable) by which your nodes can reach each other

### Although ClickHouse tracks changes in the config files and reloads users and clusters configurations on the fly, some changes it the configuration may
### require a restart, such as changing/adding listening ports. Since it's a rare case and in order to avoid unwanted server restarts, this action can only be
### triggered manually by setting the variable below to 'true'. DO NOT set it to 'true' in your VARS file, instead - pass it as an extra variable to
### 'ansible-playbook' only when you need to. Example 'ansible-playbook playbook.yml -e "{'clickhouse_force_restart': true}" '
clickhouse_force_restart: false

##########################
###     DISK MOUNTS    ###
##########################
clickhouse_main_mount_dir: "/srv"
clickhouse_disk_mounts:
  - src: "/dev/sdb"
    dest: "{{ clickhouse_main_mount_dir }}"
    owner: "root"
    group: "root"
    mode: "0755"
    fstype: "ext4"
    fs_opts: "-m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard"
    mount_opts: "discard,defaults,nofail"
    state: mounted

#############################
###     SYSTEM CONFIGS    ###
#############################
clickhouse_sysctl_config: [
  { name: "vm.max_map_count", value: "262144", state: "present" }
]

### Dictionary
clickhouse_limits_host:
  max_open_files: 500000    # default value

### List
clickhouse_limits_docker:
  - "nofile:500000:500000"

clickhouse_linux_capabilities_host:
  - "CAP_NET_ADMIN"
  - "CAP_IPC_LOCK"
  - "CAP_SYS_NICE"

clickhouse_linux_capabilities_docker:
  - "NET_ADMIN"
  - "IPC_LOCK"
  - "SYS_NICE"

### By default the official Yandex ClickHouse Docker image uses UID:GID 101:101. The user/group name does not matter, we just need to make sure the ClickHouse directories on the host machine are owned by the correct UID and GID  
clickhouse_owner_docker:
  name: ""
  uid: "101"

clickhouse_group_docker:
  name: ""
  gid: "101"

####################################################
###     CLICKHOUSE LISTEN ADDRESSES AND PORTS    ###
####################################################
clickhouse_http_port: 8123
clickhouse_https_port: 8443
clickhouse_tcp_port: 9000
clickhouse_tcp_port_secure: 9440
clickhouse_interserver_http_port: 9009
clickhouse_interserver_https_port: 9449

###################################
###     CLICKHOUSE ENDPOINTS    ###
###################################
# this URL (IP or hostname) is used to check if ClickHouse is alive
clickhouse_http_healthcheck_host: "http://127.0.0.1"
# 'local' - execute post installation queries on one of the CH nodes; 'remote' (!!!NOT YET READY!!!) - execute post installation queries on the CI/CD server
clickhouse_cli_connection_type: "local"
# this IP/host address is used for performing post installation tasks via 'clickhouse-client'
clickhouse_cli_host: "http://clickhouse-load-balancer.example.local"
# Hide Ansible output for 'clickhouse-client' commands. You may disable it if you use a local user (only allowed to connect from 127.0.0.1) that has no password
clickhouse_cli_no_log: true

##################################
###     CLICKHOUSE BINARIES    ###
##################################
clickhouse_config_validation_bin: "clickhouse-extract-from-config"

##############################################################
###     CLICKHOUSE TEMPLATES (ANSIBLE JINJA2 TEMPLATES)    ###
##############################################################
### If you want to use custom config files, feel free to change this variable to your constom directory, say, "{{ playbook_dir }}/config-templates/clickhouse"
clickhouse_server_templates_dir: "clickhouse-server"
clickhouse_client_templates_dir: "clickhouse-client"

###################################################
###     CLICKHOUSE DIRECTORIES [DOCKER HOST]    ###
###################################################
clickhouse_main_data_dir: "{{ clickhouse_main_mount_dir }}/clickhouse-data"
clickhouse_main_server_config_dir: "{{ clickhouse_main_mount_dir }}/clickhouse-server-config"
clickhouse_include_server_config_dir: "{{ clickhouse_main_server_config_dir }}/conf.d"
clickhouse_main_client_config_dir: "{{ clickhouse_main_mount_dir }}/clickhouse-client-config"
clickhouse_include_client_config_dir: "{{ clickhouse_main_client_config_dir }}/conf.d"
clickhouse_ssl_dir: "{{ clickhouse_main_server_config_dir }}/ssl"
clickhouse_log_dir: "{{ clickhouse_main_mount_dir }}/clickhouse-logs"

clickhouse_data_dir: "{{ clickhouse_main_data_dir }}/data"
clickhouse_shadow_dir: "{{ clickhouse_main_data_dir }}/shadow"
clickhouse_access_dir: "{{ clickhouse_main_data_dir }}/access"
clickhouse_prep_conf_dir: "{{ clickhouse_main_data_dir }}/preprocessed_configs"
clickhouse_store_dir: "{{ clickhouse_main_data_dir }}/store"
clickhouse_metadata_dir: "{{ clickhouse_main_data_dir }}/metadata"
clickhouse_tmp_dir: "{{ clickhouse_main_data_dir }}/tmp"
clickhouse_format_schemas_dir: "{{ clickhouse_main_data_dir }}/format_schemas"
#clickhouse_ssl_dir: "{{ clickhouse_main_data_dir }}/ssl"

##############################################
###     CLICKHOUSE DIRECTORIES [DOCKER]    ###
##############################################

# Define default host directories in case there is no Ansible Vars file for the current Linux distribution
clickhouse_default_main_dir: "/var/lib/clickhouse"
#clickhouse_default_ssl_dir: "{{ clickhouse_default_main_dir }}/ssl"
clickhouse_default_main_server_config_dir: "/etc/clickhouse-server"
clickhouse_default_ssl_dir: "{{ clickhouse_default_main_server_config_dir }}/ssl"
clickhouse_default_include_server_config_dir: "{{ clickhouse_default_main_server_config_dir }}/conf.d"
clickhouse_default_main_client_config_dir: "/etc/clickhouse-client"
clickhouse_default_include_client_config_dir: "{{ clickhouse_default_main_client_config_dir }}/conf.d"
clickhouse_default_log_dir: "/var/log/clickhouse-server"

clickhouse_directories_container:
### CONFIG DIR
  - host_dir: "{{ clickhouse_main_server_config_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_include_server_config_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_ssl_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_main_client_config_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_include_client_config_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
#### DATA DIR
  - host_dir: "{{ clickhouse_main_data_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
### LOG DIR
  - host_dir: "{{ clickhouse_log_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
### DATA SUBDIRS
  - host_dir: "{{ clickhouse_data_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_shadow_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_access_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_prep_conf_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_store_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_metadata_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  - host_dir: "{{ clickhouse_tmp_dir }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0750"
  # - host_dir: "{{ clickhouse_ssl_dir }}"
  #   owner: "{{ clickhouse_owner_docker['uid'] }}"
  #   group: "{{ clickhouse_group_docker['gid'] }}"
  #   mode: "0750"

#####################################################################
###     CLICKHOUSE FILES (YOU'D BETTER NOT CHANGE THEIR NAMES)    ###
#####################################################################
clickhouse_main_config_file: "config.xml"
clickhouse_remote_servers_config_file: "clickhouse-remote-servers.xml"
clickhouse_dicts_config_file: "auto-dictionary.xml"
clickhouse_macros_config_file: "macros.xml"
clickhouse_users_config_file: "users.xml"
clickhouse_zookeeper_config_file: "zookeeper-servers.xml"
clickhouse_compression_config_file: "clickhouse-compression.xml"
clickhouse_main_client_config_file: "config.xml"

clickhouse_files_container:
### MAIN CONFIG DIR
  - src: "{{ clickhouse_server_templates_dir }}/{{ clickhouse_main_config_file }}.j2"
    dest: "{{ clickhouse_main_server_config_dir }}/{{ clickhouse_main_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
  - src: "{{ clickhouse_server_templates_dir }}/{{ clickhouse_users_config_file }}.j2"
    dest: "{{ clickhouse_main_server_config_dir }}/{{ clickhouse_users_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
  - src: "{{ clickhouse_server_templates_dir }}/{{ clickhouse_dicts_config_file }}.j2"
    dest: "{{ clickhouse_main_server_config_dir }}/{{ clickhouse_dicts_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
### INCLUDE CONFIG DIR: 'conf.d'
  - src: "{{ clickhouse_server_templates_dir }}/conf.d/{{ clickhouse_remote_servers_config_file }}.j2"
    dest: "{{ clickhouse_include_server_config_dir }}/{{ clickhouse_remote_servers_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
  - src: "{{ clickhouse_server_templates_dir }}/conf.d/{{ clickhouse_zookeeper_config_file }}.j2"
    dest: "{{ clickhouse_include_server_config_dir }}/{{ clickhouse_zookeeper_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
  - src: "{{ clickhouse_server_templates_dir }}/conf.d/{{ clickhouse_macros_config_file }}.j2"
    dest: "{{ clickhouse_include_server_config_dir }}/{{ clickhouse_macros_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
  - src: "{{ clickhouse_server_templates_dir }}/conf.d/{{ clickhouse_compression_config_file }}.j2"
    dest: "{{ clickhouse_include_server_config_dir }}/{{ clickhouse_compression_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"
### CLIENT CONFIG DIR
  - src: "{{ clickhouse_client_templates_dir }}/{{ clickhouse_main_client_config_file }}.j2"
    dest: "{{ clickhouse_main_client_config_dir }}/{{ clickhouse_main_client_config_file }}"
    owner: "{{ clickhouse_owner_docker['uid'] }}"
    group: "{{ clickhouse_group_docker['gid'] }}"
    mode: "0440"

### CONFIG FILES VALIDATION
### Syntax: clickhouse-extract-from-config --config-file </path/to/config/file> --key=<config_file_key>
### Example: clickhouse-extract-from-config --config-file /etc/clickhouse-server/config.xml --key=path
### If the config file has a syntax error, you will get an error when you try to export any of the its keys
clickhouse_validate_config_files:
- config_file: "{{ clickhouse_default_main_server_config_dir }}/{{ clickhouse_main_config_file }}"
  config_key: "path"
- config_file: "{{ clickhouse_default_include_server_config_dir }}/{{ clickhouse_remote_servers_config_file }}"
  config_key: "remote_servers"
- config_file: "{{ clickhouse_default_main_server_config_dir }}/{{ clickhouse_users_config_file }}"
  config_key: "users"
- config_file: "{{ clickhouse_default_include_server_config_dir }}/{{ clickhouse_zookeeper_config_file }}"
  config_key: "zookeeper"
- config_file: "{{ clickhouse_default_include_server_config_dir }}/{{ clickhouse_macros_config_file }}"
  config_key: "macros"
- config_file: "{{ clickhouse_default_include_server_config_dir }}/{{ clickhouse_compression_config_file }}"
  config_key: "compression"
- config_file: "{{ clickhouse_default_main_server_config_dir }}/{{ clickhouse_dicts_config_file }}"
  config_key: "dictionaries"

#clickhouse_version: "20.3.15"        # Docker tag
clickhouse_version: "20.3.15.133"    # Ubuntu APT package

################################
###     CLICKHOUSE CONFIG    ###
################################
clickhouse_docker_network: "clickhouse"

clickhouse_server_config:
  max_connections: 2048
  keep_alive_timeout: 3
  max_concurrent_queries: 100
  uncompressed_cache_size: 8589934592
  mark_cache_size: 5368709120
  builtin_dictionaries_reload_interval: 3600
  max_session_timeout: 3600
  default_session_timeout: 60
  max_table_size_to_drop: 10000000000

# clickhouse_compression:
# - min_part_size: 10000000000
#   min_part_size_ratio: "0.01"
#   method: "zstd"

clickhouse_docker_env_vars:
  CLICKHOUSE_CONFIG: "/etc/clickhouse-server/config.xml"

### You may needs this if you want to run ClickHouse in Docker on a Ubuntu host with AppArmor enabled (otherwise, you won't be able to add Linux Capabilities)
clickhouse_docker_security_opts:
  - "apparmor:unconfined"

clickhouse_docker_log_options:
  max-file: 10
  max-size: "100m"

clickhouse_config:
  global:
    major_version: "{{ clickhouse_version.split('.')[0] }}"
    status_check_retries: "20"  # a number of retries when checking the node status
    status_check_delay: "15"    # a delay in seconds when checking the node status
  docker_opts:
    container_name: "clickhouse"
    image_name: "yandex/clickhouse-server"
    image_tag: "{{ clickhouse_version }}"
    command: "{{ clickhouse_command | default(omit) }}"
    ports:
      - "{{ clickhouse_http_port }}:{{ clickhouse_http_port }}"
      - "{{ clickhouse_https_port }}:{{ clickhouse_https_port }}"
      - "{{ clickhouse_tcp_port }}:{{ clickhouse_tcp_port }}"
      - "{{ clickhouse_tcp_port_secure }}:{{ clickhouse_tcp_port_secure }}"
      - "{{ clickhouse_interserver_http_port }}:{{ clickhouse_interserver_http_port }}"
      - "{{ clickhouse_interserver_https_port }}:{{ clickhouse_interserver_https_port }}"
    env_vars: "{{ clickhouse_docker_env_vars | default(omit) }}"
    volumes:
      - "{{ clickhouse_main_data_dir }}:{{ clickhouse_default_main_dir }}"
      - "{{ clickhouse_main_server_config_dir }}:{{ clickhouse_default_main_server_config_dir }}"
      - "{{ clickhouse_main_client_config_dir }}:{{ clickhouse_default_main_client_config_dir }}"
      - "{{ clickhouse_log_dir }}:{{ clickhouse_default_log_dir }}"
    hostname: "{{ clickhouse_node_hostname }}"
    networks:
      - name: "{{ clickhouse_docker_network }}"
    network_mode: "{{ clickhouse_docker_network_mode | default(omit) }}"
    log_options: "{{ clickhouse_docker_log_options | default(omit) }}"
    ulimits: "{{ clickhouse_limits_docker | default(omit) }}"
    capabilities: "{{ clickhouse_linux_capabilities_docker | default(omit) }}"
    security_opts: "{{ clickhouse_docker_security_opts | default(omit) }}"

########################################################################
###     CLICKHOUSE REMOTE SERVERS (clickhouse-remote-servers.xml)    ###
########################################################################
### Whether to write data to just one of the replicas. Default: false (write data to all replicas)
clickhouse_internal_replication: "false"
### Use ssl for connection, usually you also should define port = 9440. Server should listen on <tcp_port_secure>9440</tcp_port_secure> and have correct certificates.
clickhouse_interserver_tcp_ssl: false
### If a user is not set, and '<secret></secret>' is defined, then the initial_user (the user who initiated the query) should be used

clickhouse_interserver_distributed_auth_enabled: true    # requires 'clickhouse_interserver_user' (the user should exist in the users.xml, ClickHouse knows how to get its password)
clickhouse_interserver_http_auth_enabled: true    # requires 'interserver_http_auth_user' and 'interserver_http_auth_password'

################################
###     SSL CERTIFICATES     ###
################################
clickhouse_generate_self_signed_ssl: false

clickhouse_root_ca_cert_name: "ca.crt"
clickhouse_root_ca_key_name: "ca.key"
clickhouse_node_csr_name: "server.csr"
clickhouse_node_cert_name: "server.crt"
clickhouse_node_key_name: "server.key"
clickhouse_node_dhparam_name: "dhparam.pem"

### Shell command for generating a Root CA and key: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out root-ca.pem -days 7300 -nodes -subj "/C=US/ST=NY/L=NewYork/O=MyOrganization/OU=IT/CN=clickhouse.local/emailAddress=admin@clickhouse.local"
clickhouse_openssl:
  valid_to: "+365d"
  csr:
    common_name: "clickhouse.local"
    country_name: "US"
    state_or_province_name: "NY"
    locality_name: "NY"
    organizational_unit_name: "IT"
    organization_name: "MyOrganization"
    email_address: "admin@clickhouse.local"
    subject_alt_name:
      - "DNS:{{ clickhouse_node_hostname }}"
      - "IP:{{ ansible_host }}"

clickhouse_ssl:
### Default HTTP interface
  http_interface_ssl_enable: false
  http_interface_ssl_only: false
### Native interface
  native_interface_ssl_enable: false
  native_interface_ssl_only: false
### Same Native interface but for dictributed queries
  distributed_query_ssl_enable: false
### Communication between replicas. Used for data exchange
  interserver_repl_ssl_enable: false    # if enabled, you must set the `clickhouse_interserver_https_host` too

#################################################
###     CLICKHOUSE SERVER LISTEN ADDRESSES    ###
#################################################
### It is recommended to keep Listen Host empty in order for ClickHouse nodes to be able to communicate with each other
clickhouse_listen_host_default: []
#  - "::1"
#  - "127.0.0.1"
clickhouse_listen_host_custom:
  - "0.0.0.0"
clickhouse_listen_host: "{{ clickhouse_listen_host_default + clickhouse_listen_host_custom }}"

##################################
###     CLICKHOUSE PROFILES    ###
##################################
clickhouse_profiles_default:
  default:
    max_memory_usage: 10000000000
    use_uncompressed_cache: 0
    load_balancing: random
  readonly:
    readonly: 1

clickhouse_profiles_custom: ""

####################################################################
###     CLICKHOUSE IPs/SUBNETS TO ALLOW USERS TO CONNECT FROM    ###
####################################################################
clickhouse_networks_default:
  - "127.0.0.1"

### If you want to restrict direct access to ClickHouse nodes and only allow access via your Load Balancer, then whitelist your Load Balancer IP/subnet
clickhouse_networks_custom:
  gcp-internal:
    - "10.128.0.0/9"
  docker:
    - "172.16.0.0/12"

##########################################
###     CLICKHOUSE USERS AND QUOTAS    ###
##########################################
clickhouse_quotas_intervals_default: [
  { duration: 3600, queries: 0, errors: 0,result_rows: 0,read_rows: 0,execution_time: 0 }
]

clickhouse_quotas_default: [
  { name: "default", intervals: "{{ clickhouse_quotas_intervals_default }}", comment: "Default quota - count only" }
]

clickhouse_quotas_custom: []

clickhouse_users_default:
  - name: "default"
    password: ""
    networks: "{{ clickhouse_networks_default }}"
    profile: "default"
    quota: "default"
    comment: "Default user for login if user not defined"
    dbs:
      - "default"
      - "system"
### the 'system_user' options allows your user to connect from one CH node to another by adding the CH nodes' IPs to <networks></networks>
#    system_user: true
  - name: "readonly"
    password: ""
    networks: "{{ clickhouse_networks_default }}"
    profile: "default"
    quota: "default"
    comment: "Example of user with readonly access"
    dbs:
      - "default"

clickhouse_users_custom: []
#  - name: "monitoring"
#    password: "VerySecurePassword"
#    networks: "{{ clickhouse_networks_custom['gcp-internal'] + clickhouse_networks_custom['docker'] }}"
#    profile: "readonly"
#    quota: "default"
#    comment: "Monitoring user with read only access"


###########################################
###     CLICKHOUSE ADDITIONAL CONFIGS   ###
###########################################
clickhouse_logger:
  server_log:
    level: trace
    log: "{{ clickhouse_default_log_dir }}/clickhouse-server.log"
    errorlog: "{{ clickhouse_default_log_dir }}/clickhouse-server.err.log"
    size: 1000M
    count: 10
  query_log:
    database: "system"
    table: "query_log"
    partition_by: "toYYYYMM(event_date)"
    flush_interval_milliseconds: 7500
  # part_log:
  #   database: "system"
  #   table: "part_log"
  #   flush_interval_milliseconds: 7500

clickhouse_dicts: []

clickhouse_zookeeper_config:
  resharding:
    task_queue_path: "/clickhouse/{{ clickhouse_cluster_name }}/task_queue"
  ### Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster. Works only if ZooKeeper is enabled. Comment it if such functionality isn't required  
  distributed_ddl:
    path: "/clickhouse/{{ clickhouse_cluster_name }}/task_queue/ddl"

###################################
###     CLICKHOUSE MONITORING   ###
###################################
# clickhouse_monitoring:
#   type: "prometheus"
#   config_opts:
#     endpoint: "/metrics"
#     port: "8001"
#     metrics: true
#     events: true
#     asynchronous_metrics: true

# clickhouse_monitoring:
#   type: "graphite"
#   config_opts:
#     host: "localhost"
#     port: 42000
#     timeout: "0.1"
#     interval: 60
#     root_path: "one_min"
#     metrics: true
#     events: true
#     events_cumulative: false
#     asynchronous_metrics: true

################################################
###     CLICKHOUSE POST INSTALLATION TASKS   ###
################################################

### DATABASES
clickhouse_dbs_default: []
clickhouse_dbs_custom: []

clickhouse_dbs: "{{ clickhouse_dbs_default + clickhouse_dbs_custom }}"

### CUSTOM SQL SCRIPTS
clickhouse_custom_sql_scripts:
  path_local: "{{ playbook_dir }}/sql-scripts"    # local path that will be searched for SQL scripts; if used with 'git', this path should match with the git destination directory or be its subdirectory
  ### If 'path_remote' is set, then Ansible will copy your SQL scripts FROM the local machine TO one of the remote ClickHouse nodes for further execution of SQL queries one the 'ClickHouse localhost'
  path_remote: "/tmp/custom_sql_scripts"    # it 'path_remote' is defined, the SQL scripts will be copied to a remote ClickHouse node for further execution on that node; otherwise, the SQL scripts will be executed on the machine where Ansible is running (make sure it has permissison to execute ClickHouse commands)
  search_patterns: ["*.sql"]    # search for patterns
  search_excludes: ["*_test.sql"]    # exclude patterns from search results
  use_regex: false    # false - use Shell regex, true - use Python regex
  search_recursively: true    # search for SQL scripts in the directory and its subdirectories
  sql_scripts_are_jinja: true    # when SQL scripts are transferred to the remote ClickHouse server for execution, they will be interpreted as Jinja templates, thus '{{ }}' will be interpreted as variables
  ### if 'git' is set, then Ansible will attempt to clone the git repo with your SQL scripts to the local directory
  git:
    repo: "git@github.com:myteam/my-repo.git"
    dest: "{{ playbook_dir }}/sql-scripts"
    version: "master"
    accept_hostkey: true
    key_file: "{{ git_ssh_key_path | default(omit) }}"